{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Introduction\n",
    "In this notebook, we aim to predict how a brand will rank on the interbrand ranking based on its past ranking, frequency, and word embedding.\n",
    "\n",
    "The following files are needed to run the codes:\n",
    "\n",
    "1. the word embedding model file: `L10T50G100A1ngV_iter1.p` (sent via google drive because it is too large for github)\n",
    "2. the word to id json: `w2id_glove_corpora_minc_100.json` (on github)\n",
    "3. the interbrand ranking by year json: `interbrand_brand2rankvalue.json` (on github)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import scipy.spatial\n",
    "import scipy.linalg\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the sample code for using the word embedding model\n",
    "# model file path\n",
    "# modify according to your setup\n",
    "file_dir = '/Users/vincent/GoogleDriveSync/NTUCourse/BerkeleyCourse/Ming Hsu Lab/various_embedding/pierre2/stereotyping_word2vec_nodata/scraping/nytimes_2011/'\n",
    "dw2v_filepath = file_dir + 'L10T50G100A1ngV_iter4.p' \n",
    "w2id_path = file_dir + 'w2id_glove_corpora_1996-2019_minc_100.json'\n",
    "\n",
    "# define dynamic word vec class \n",
    "class DynamicWordVec():\n",
    "    def __init__(self, dw2v_filepath, w2id_path):\n",
    "        with open(dw2v_filepath, 'rb') as f:\n",
    "            self.wordvec_matrix = pickle.load(f)\n",
    "            self.num_periods = len(self.wordvec_matrix)\n",
    "\n",
    "        with open(w2id_path) as f:\n",
    "            self.w2id = json.load(f)\n",
    "    \n",
    "    def get_vec(self, w, yr):\n",
    "        return self.wordvec_matrix[yr][self.w2id[w], :]\n",
    "\n",
    "    def sim_by_vec(self, v1, v2, sim_type='cosim'):\n",
    "        if sim_type == 'inner':\n",
    "            return np.inner(v1, v2)\n",
    "        elif sim_type == 'cosim':\n",
    "            return 1 - sp.spatial.distance.cosine(v1, v2)\n",
    "        else:\n",
    "            raise Exception('sim_type should be either \"inner\" or \"cosim\"')\n",
    "    \n",
    "    def sim_by_word_year(self, w1, y1, w2, y2, sim_type='cosim'):\n",
    "        v1 = self.wordvec_matrix[y1][self.w2id[w1], :]\n",
    "        v2 = self.wordvec_matrix[y2][self.w2id[w2], :]\n",
    "        return self.sim_by_vec(v1, v2, sim_type=sim_type)\n",
    "    \n",
    "    def is_in_vocab(self, w):\n",
    "        return w in self.w2id\n",
    "    \n",
    "    def most_similar_words_in_year(self, w1, y1, topn, y2, include_self, sim_type='cosim'):\n",
    "        temp_dict = {}\n",
    "        for word in self.w2id:\n",
    "            cont = True\n",
    "            if not include_self:\n",
    "                if word == w1:\n",
    "                    cont = False\n",
    "            if cont:\n",
    "                temp_dict[word] = self.sim_by_word_year(w1, y1, word, y2)\n",
    "        return sorted(temp_dict.items(), key=lambda x: x[1], reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1996 ['obama(0.96)', 'obama’s(0.91)', 'president’s(0.83)', 'bush(0.82)', 'clinton(0.82)', 'congress(0.81)', 'campaign(0.80)', 'administration(0.80)', 'republicans(0.79)', 'support(0.78)']\n1997 ['obama(0.96)', 'obama’s(0.92)', 'clinton(0.85)', 'donald(0.83)', 'trump(0.83)', 'trump’s(0.83)', 'bush(0.83)', 'campaign(0.82)', 'hillary(0.82)', 'presidentelect(0.82)']\n1998 ['obama(0.91)', 'trump’s(0.89)', 'trump(0.87)', 'president’s(0.85)', 'obama’s(0.84)', 'barack(0.84)', 'administration(0.83)', 'presidentelect(0.81)', 'donald(0.80)', 'president(0.80)']\n1999 ['trump(0.88)', 'trump’s(0.88)', 'obama(0.86)', 'president’s(0.83)', 'president(0.82)', 'administration(0.79)', 'congress(0.78)', 'presidential(0.78)', 'barack(0.76)', 'republicans(0.76)']\n2000 ['trump(0.87)', 'trump’s(0.86)', 'president(0.81)', 'pelosi(0.81)', 'obama(0.81)', 'biden(0.79)', 'democrats(0.79)', 'president’s(0.79)', 'presidential(0.77)', 'congress(0.77)']\n"
     ]
    }
   ],
   "source": [
    "dw2v = DynamicWordVec(dw2v_filepath, w2id_path) # load the trained dynamic word embedding model\n",
    "\n",
    "start_t = 1996 # the first year of the corpora\n",
    "target_word = 'obama' # target word of your choice\n",
    "target_word_yr = 2011 # the year of the target word, again, feel free to change it\n",
    "\n",
    "result_word_yrs = range(2015, 2020) \n",
    "\n",
    "most_sim_words = [dw2v.most_similar_words_in_year(target_word, target_word_yr-start_t, 10, yr-start_t, True) for yr in result_word_yrs]\n",
    "for i, items in enumerate(most_sim_words):\n",
    "    # this example gets the most similar word of twitter-2011 in each year\n",
    "    string_to_print = ['{}({:.2f})'.format(w, sim) for w, sim in items]\n",
    "    print(i+start_t, string_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "read files\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "## Create dataset\n",
    "\n",
    "print('read files')\n",
    "dw2v = DynamicWordVec(dw2v_filepath, w2id_path)\n",
    "\n",
    "with open('./interbrand_brand2freq.json') as f:\n",
    "    brand2year2freq = json.load(f)\n",
    "\n",
    "with open('./interbrand_brand2rankvalue.json') as f:\n",
    "    brand2year2rankvalue = json.load(f)\n",
    "\n",
    "with open('./glove_corpora_totalwordcount.json') as f:\n",
    "    year2count = json.load(f)\n",
    "\n",
    "brands = sorted(list(brand2year2freq))\n",
    "brands = [b for b in brands if dw2v.is_in_vocab(b)]\n",
    "\n",
    "Ts = range(2001, 2019)\n",
    "corpurs_start_t = 1996\n",
    "rank_if_not_list = 101\n",
    "n_feature = 67\n",
    "\n",
    "dataset = []\n",
    "header = ['brand_year', 'brand', 'year'] + ['wordvec_{}'.format(i) for i in range(1, 51)] + \\\n",
    "    ['freq_t-4', 'freq_t-3', 'freq_t-2', 'freq_t-1', 'freq_t'] + \\\n",
    "    ['ratiopermille_t-4', 'ratiopermille_t-3', 'ratiopermille_t-2', 'ratiopermille_t-1', 'ratiopermille_t'] + \\\n",
    "    ['rank_t', 'rank_t+1', 'isonlist_t', 'isonlist_t+1']\n",
    "print(len(header))\n",
    "assert(len(header)==n_feature)\n",
    "for brand in brands:\n",
    "    for t in Ts:\n",
    "        #print(brand, t)\n",
    "        wordvec = dw2v.get_vec(brand, t-corpurs_start_t).tolist() # dim-50\n",
    "        freq_tm4_to_t = [brand2year2freq[brand][str(s)] for s in range(t-4, t+1)]\n",
    "        totalwordcount_tm4_to_t = [year2count[str(s)] for s in range(t-4, t+1)]\n",
    "        ratiopermille_tm4_to_t = [freq*1000 / count for freq, count in zip(freq_tm4_to_t, totalwordcount_tm4_to_t)]\n",
    "\n",
    "        rank_t = brand2year2rankvalue[brand].get(str(t), (rank_if_not_list,))[0] # if not on the list, rank=101\n",
    "        rank_tp1 = brand2year2rankvalue[brand].get(str(t+1), (rank_if_not_list,))[0]\n",
    "\n",
    "        ison_t = rank_t < rank_if_not_list\n",
    "        ison_tp1 = rank_tp1 < rank_if_not_list\n",
    "\n",
    "        brandyear = '{}-{}'.format(brand, t)\n",
    "\n",
    "        current_row = [brandyear, brand, t] + wordvec + freq_tm4_to_t + ratiopermille_tm4_to_t + [rank_t, rank_tp1, ison_t, ison_tp1]\n",
    "        current_row = [str(c) for c in current_row]\n",
    "        assert(len(current_row)==n_feature)\n",
    "        dataset.append(current_row)\n",
    "\n",
    "\n",
    "dataset = [','.join(row)+'\\n' for row in dataset]\n",
    "dataset = [','.join(header)+'\\n'] + dataset\n",
    "with open('interbrand_dataset.csv', 'w') as f:\n",
    "    for row in dataset:\n",
    "        f.write(row)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}