{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Introduction\n",
    "In this notebook, we aim to predict how a brand will rank on the interbrand ranking based on its past ranking, frequency, and word embedding.\n",
    "\n",
    "The following files are needed to run the codes:\n",
    "\n",
    "1. the word embedding model file: `L10T50G100A1ngV_iter1.p` (sent via google drive because it is too large for github)\n",
    "2. the word to id json: `w2id_glove_corpora_minc_100.json` (on github)\n",
    "3. the interbrand ranking by year json: `interbrand_brand2rankvalue.json` (on github)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import scipy.spatial\n",
    "import scipy.linalg\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error, plot_roc_curve, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the sample code for using the word embedding model\n",
    "# model file path\n",
    "# modify according to your setup\n",
    "file_dir = '/Users/vincent/GoogleDriveSync/NTUCourse/BerkeleyCourse/Ming Hsu Lab/various_embedding/pierre2/stereotyping_word2vec_nodata/scraping/nytimes_2011/'\n",
    "dw2v_filepath = file_dir + 'L10T50G100A1ngV_iter4.p' \n",
    "w2id_path = file_dir + 'w2id_glove_corpora_1996-2019_minc_100.json'\n",
    "\n",
    "# define dynamic word vec class \n",
    "class DynamicWordVec():\n",
    "    def __init__(self, dw2v_filepath, w2id_path):\n",
    "        with open(dw2v_filepath, 'rb') as f:\n",
    "            self.wordvec_matrix = pickle.load(f)\n",
    "            self.num_periods = len(self.wordvec_matrix)\n",
    "\n",
    "        with open(w2id_path) as f:\n",
    "            self.w2id = json.load(f)\n",
    "    \n",
    "    def get_vec(self, w, yr):\n",
    "        return self.wordvec_matrix[yr][self.w2id[w], :]\n",
    "\n",
    "    def sim_by_vec(self, v1, v2, sim_type='cosim'):\n",
    "        if sim_type == 'inner':\n",
    "            return np.inner(v1, v2)\n",
    "        elif sim_type == 'cosim':\n",
    "            return 1 - sp.spatial.distance.cosine(v1, v2)\n",
    "        else:\n",
    "            raise Exception('sim_type should be either \"inner\" or \"cosim\"')\n",
    "    \n",
    "    def sim_by_word_year(self, w1, y1, w2, y2, sim_type='cosim'):\n",
    "        v1 = self.wordvec_matrix[y1][self.w2id[w1], :]\n",
    "        v2 = self.wordvec_matrix[y2][self.w2id[w2], :]\n",
    "        return self.sim_by_vec(v1, v2, sim_type=sim_type)\n",
    "    \n",
    "    def is_in_vocab(self, w):\n",
    "        return w in self.w2id\n",
    "    \n",
    "    def most_similar_words_in_year(self, w1, y1, topn, y2, include_self, sim_type='cosim'):\n",
    "        temp_dict = {}\n",
    "        for word in self.w2id:\n",
    "            cont = True\n",
    "            if not include_self:\n",
    "                if word == w1:\n",
    "                    cont = False\n",
    "            if cont:\n",
    "                temp_dict[word] = self.sim_by_word_year(w1, y1, word, y2)\n",
    "        return sorted(temp_dict.items(), key=lambda x: x[1], reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dw2v = DynamicWordVec(dw2v_filepath, w2id_path) # load the trained dynamic word embedding model\n",
    "\n",
    "start_t = 1996 # the first year of the corpora\n",
    "target_word = 'obama' # target word of your choice\n",
    "target_word_yr = 2011 # the year of the target word, again, feel free to change it\n",
    "\n",
    "result_word_yrs = range(2015, 2020) \n",
    "\n",
    "most_sim_words = [dw2v.most_similar_words_in_year(target_word, target_word_yr-start_t, 10, yr-start_t, True) for yr in result_word_yrs]\n",
    "for i, items in enumerate(most_sim_words):\n",
    "    # this example gets the most similar word of twitter-2011 in each year\n",
    "    string_to_print = ['{}({:.2f})'.format(w, sim) for w, sim in items]\n",
    "    print(i+start_t, string_to_print)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create dataset\n",
    "\n",
    "print('read files')\n",
    "dw2v = DynamicWordVec(dw2v_filepath, w2id_path)\n",
    "\n",
    "with open('./interbrand_brand2freq.json') as f:\n",
    "    brand2year2freq = json.load(f)\n",
    "\n",
    "with open('./interbrand_brand2rankvalue.json') as f:\n",
    "    brand2year2rankvalue = json.load(f)\n",
    "\n",
    "with open('./glove_corpora_totalwordcount.json') as f:\n",
    "    year2count = json.load(f)\n",
    "\n",
    "brands = sorted(list(brand2year2freq))\n",
    "brands = [b for b in brands if dw2v.is_in_vocab(b)]\n",
    "\n",
    "Ts = range(2001, 2019)\n",
    "corpurs_start_t = 1996\n",
    "rank_if_not_list = 101\n",
    "n_feature = 67\n",
    "\n",
    "dataset = []\n",
    "header = ['brand_year', 'brand', 'year'] + ['wordvec_{}'.format(i) for i in range(1, 51)] + \\\n",
    "    ['freq_t-4', 'freq_t-3', 'freq_t-2', 'freq_t-1', 'freq_t'] + \\\n",
    "    ['ratiopermille_t-4', 'ratiopermille_t-3', 'ratiopermille_t-2', 'ratiopermille_t-1', 'ratiopermille_t'] + \\\n",
    "    ['rank_t', 'rank_t+1', 'isonlist_t', 'isonlist_t+1']\n",
    "print(len(header))\n",
    "assert(len(header)==n_feature)\n",
    "for brand in brands:\n",
    "    for t in Ts:\n",
    "        #print(brand, t)\n",
    "        wordvec = dw2v.get_vec(brand, t-corpurs_start_t).tolist() # dim-50\n",
    "        freq_tm4_to_t = [brand2year2freq[brand][str(s)] for s in range(t-4, t+1)]\n",
    "        totalwordcount_tm4_to_t = [year2count[str(s)] for s in range(t-4, t+1)]\n",
    "        ratiopermille_tm4_to_t = [freq*1000 / count for freq, count in zip(freq_tm4_to_t, totalwordcount_tm4_to_t)]\n",
    "\n",
    "        rank_t = brand2year2rankvalue[brand].get(str(t), (rank_if_not_list,))[0] # if not on the list, rank=101\n",
    "        rank_tp1 = brand2year2rankvalue[brand].get(str(t+1), (rank_if_not_list,))[0]\n",
    "\n",
    "        ison_t = rank_t < rank_if_not_list\n",
    "        ison_tp1 = rank_tp1 < rank_if_not_list\n",
    "\n",
    "        brandyear = '{}-{}'.format(brand, t)\n",
    "\n",
    "        current_row = [brandyear, brand, t] + wordvec + freq_tm4_to_t + ratiopermille_tm4_to_t + [rank_t, rank_tp1, ison_t, ison_tp1]\n",
    "        current_row = [str(c) for c in current_row]\n",
    "        assert(len(current_row)==n_feature)\n",
    "        dataset.append(current_row)\n",
    "\n",
    "\n",
    "dataset = [','.join(row)+'\\n' for row in dataset]\n",
    "dataset = [','.join(header)+'\\n'] + dataset\n",
    "with open('interbrand_dataset.csv', 'w') as f:\n",
    "    for row in dataset:\n",
    "        f.write(row)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid/test split\n",
    "\n",
    "df = pd.read_csv('interbrand_dataset.csv')\n",
    "\n",
    "# consider only brands with 1000+ freq\n",
    "#brand1000p = [b for b in brands if any([brand2year2freq[b][str(y)]>500 for y in range(2001, 2019)])]\n",
    "#df = df.loc[df['brand'].isin(brand1000p)]\n",
    "\n",
    "n_data = len(df)\n",
    "n_train = int(n_data*0.7)\n",
    "n_valid = int(n_data*0.2)\n",
    "n_test = n_data - n_train - n_valid\n",
    "assert(n_train + n_valid + n_test == n_data)\n",
    "\n",
    "print(n_train, n_valid, n_test, n_data)\n",
    "\n",
    "group_labels = np.repeat(['train', 'valid', 'test'], [n_train, n_valid, n_test])\n",
    "randomized_labels = np.random.choice(group_labels, n_data, replace=False)\n",
    "df['group'] = randomized_labels\n",
    "\n",
    "df_train = df[df['group']=='train']\n",
    "df_valid = df[df['group']=='valid']\n",
    "df_test = df[df['group']=='test']\n",
    "\n",
    "df_train = df_train.drop(columns=['group'])\n",
    "df_valid = df_valid.drop(columns=['group'])\n",
    "df_test = df_test.drop(columns=['group'])\n",
    "\n",
    "df_valid_negt = df_valid.loc[df_valid['isonlist_t'] == False]\n",
    "df_valid_post = df_valid.loc[df_valid['isonlist_t'] == True]\n",
    "print(len(df_valid_negt))\n",
    "print(len(df_valid_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict ison_t+1 using freq with logisitic regression\n",
    "feature_list_a = ['freq_t-4', 'freq_t-3', 'freq_t-2', 'freq_t-1', 'freq_t']\n",
    "\n",
    "features_a = df_train[feature_list_a].to_numpy()\n",
    "target_a = df_train['isonlist_t+1'].to_numpy()\n",
    "\n",
    "valid_features_a = df_valid[feature_list_a].to_numpy()\n",
    "valid_target_a = df_valid['isonlist_t+1'].to_numpy()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1).fit(features_a, target_a)\n",
    "hat_valid_target_a = lr.predict(valid_features_a)\n",
    "prob_valid_target_a = lr.predict_proba(valid_features_a)\n",
    "prob_valid_target_a = [p[1] for p in prob_valid_target_a]\n",
    "\n",
    "acc_a = accuracy_score(valid_target_a, hat_valid_target_a)\n",
    "auc_a = roc_auc_score(valid_target_a, prob_valid_target_a)\n",
    "\n",
    "print('acc_a: ', acc_a)\n",
    "print('auc_a: ', auc_a)\n",
    "\n",
    "plot_roc_curve(lr, valid_features_a, valid_target_a)\n",
    "plt.savefig('roc_a.png')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(valid_target_a, hat_valid_target_a, average='binary')\n",
    "print('precision:{}, recall:{}, f1:{}, support:{}'.format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_b = ['rank_t']\n",
    "\n",
    "features_b = df_train[feature_list_b].to_numpy()\n",
    "target_b = df_train['isonlist_t+1'].to_numpy()\n",
    "\n",
    "valid_features_b = df_valid[feature_list_b].to_numpy()\n",
    "valid_target_b = df_valid['isonlist_t+1'].to_numpy()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1).fit(features_b, target_b)\n",
    "hat_valid_target_b = lr.predict(valid_features_b)\n",
    "prob_valid_target_b = lr.predict_proba(valid_features_b)\n",
    "prob_valid_target_b = [p[1] for p in prob_valid_target_b]\n",
    "\n",
    "acc_b = accuracy_score(valid_target_b, hat_valid_target_b)\n",
    "auc_b = roc_auc_score(valid_target_b, prob_valid_target_b)\n",
    "\n",
    "print('acc_a: ', acc_b)\n",
    "print('auc_a: ', auc_b)\n",
    "\n",
    "plot_roc_curve(lr, valid_features_b, valid_target_b)\n",
    "plt.savefig('roc_b.png')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(valid_target_b, hat_valid_target_b, average='binary')\n",
    "print('precision:{}, recall:{}, f1:{}, support:{}'.format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_c = ['wordvec_{}'.format(i) for i in range(1, 51)] + ['freq_t-4', 'freq_t-3', 'freq_t-2', 'freq_t-1', 'freq_t']\n",
    "\n",
    "features_c = df_train[feature_list_c].to_numpy()\n",
    "target_c = df_train['isonlist_t+1'].to_numpy()\n",
    "\n",
    "valid_features_c = df_valid[feature_list_c].to_numpy()\n",
    "valid_target_c = df_valid['isonlist_t+1'].to_numpy()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1).fit(features_c, target_c)\n",
    "hat_valid_target_c = lr.predict(valid_features_c)\n",
    "prob_valid_target_c = lr.predict_proba(valid_features_c)\n",
    "prob_valid_target_c = [p[1] for p in prob_valid_target_c]\n",
    "\n",
    "acc_c = accuracy_score(valid_target_c, hat_valid_target_c)\n",
    "auc_c = roc_auc_score(valid_target_c, prob_valid_target_c)\n",
    "\n",
    "print('acc_a: ', acc_c)\n",
    "print('auc_a: ', auc_c)\n",
    "\n",
    "plot_roc_curve(lr, valid_features_c, valid_target_c)\n",
    "plt.savefig('roc_c.png')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(valid_target_c, hat_valid_target_c, average='binary')\n",
    "print('precision:{}, recall:{}, f1:{}, support:{}'.format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_c = ['wordvec_{}'.format(i) for i in range(1, 51)] + ['rank_t']\n",
    "\n",
    "features_c = df_train[feature_list_c].to_numpy()\n",
    "target_c = df_train['isonlist_t+1'].to_numpy()\n",
    "\n",
    "valid_features_c = df_valid[feature_list_c].to_numpy()\n",
    "valid_target_c = df_valid['isonlist_t+1'].to_numpy()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1).fit(features_c, target_c)\n",
    "hat_valid_target_c = lr.predict(valid_features_c)\n",
    "prob_valid_target_c = lr.predict_proba(valid_features_c)\n",
    "prob_valid_target_c = [p[1] for p in prob_valid_target_c]\n",
    "\n",
    "acc_c = accuracy_score(valid_target_c, hat_valid_target_c)\n",
    "auc_c = roc_auc_score(valid_target_c, prob_valid_target_c)\n",
    "\n",
    "print('acc_a: ', acc_c)\n",
    "print('auc_a: ', auc_c)\n",
    "\n",
    "plot_roc_curve(lr, valid_features_c, valid_target_c)\n",
    "plt.savefig('roc_c.png')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(valid_target_c, hat_valid_target_c, average='binary')\n",
    "print('precision:{}, recall:{}, f1:{}, support:{}'.format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_c = ['wordvec_{}'.format(i) for i in range(1, 51)]\n",
    "\n",
    "features_c = df_train[feature_list_c].to_numpy()\n",
    "target_c = df_train['isonlist_t+1'].to_numpy()\n",
    "\n",
    "valid_features_c = df_valid[feature_list_c].to_numpy()\n",
    "valid_target_c = df_valid['isonlist_t+1'].to_numpy()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1).fit(features_c, target_c)\n",
    "hat_valid_target_c = lr.predict(valid_features_c)\n",
    "prob_valid_target_c = lr.predict_proba(valid_features_c)\n",
    "prob_valid_target_c = [p[1] for p in prob_valid_target_c]\n",
    "\n",
    "acc_c = accuracy_score(valid_target_c, hat_valid_target_c)\n",
    "auc_c = roc_auc_score(valid_target_c, prob_valid_target_c)\n",
    "\n",
    "print('acc_a: ', acc_c)\n",
    "print('auc_a: ', auc_c)\n",
    "\n",
    "plot_roc_curve(lr, valid_features_c, valid_target_c)\n",
    "plt.savefig('roc_c.png')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(valid_target_c, hat_valid_target_c, average='binary')\n",
    "print('precision:{}, recall:{}, f1:{}, support:{}'.format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_d = ['wordvec_{}'.format(i) for i in range(1, 51)] + ['rank_t']\n",
    "\n",
    "features_d = df_train[feature_list_d].to_numpy()\n",
    "target_d = df_train['rank_t+1'].to_numpy()\n",
    "\n",
    "valid_features_d = df_valid[feature_list_d].to_numpy()\n",
    "valid_target_d = df_valid['rank_t+1'].to_numpy()\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1).fit(features_d, target_d)\n",
    "hat_valid_target_d = lr.predict(valid_features_d)\n",
    "\n",
    "mae_d = mean_absolute_error(valid_target_d, hat_valid_target_d)\n",
    "\n",
    "print('mae_a: ', mae_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_e = ['freq_t-4', 'freq_t-3', 'freq_t-2', 'freq_t-1', 'freq_t', 'rank_t']\n",
    "\n",
    "features_e = df_train[feature_list_e].to_numpy()\n",
    "target_e = df_train['rank_t+1'].to_numpy()\n",
    "\n",
    "valid_features_e = df_valid[feature_list_e].to_numpy()\n",
    "valid_target_e = df_valid['rank_t+1'].to_numpy()\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1).fit(features_e, target_e)\n",
    "hat_valid_target_e = lr.predict(valid_features_e)\n",
    "\n",
    "mae_e = mean_absolute_error(valid_target_e, hat_valid_target_e)\n",
    "\n",
    "print('mae_a: ', mae_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_f = ['rank_t']\n",
    "\n",
    "features_f = df_train[feature_list_f].to_numpy()\n",
    "target_f = df_train['rank_t+1'].to_numpy()\n",
    "\n",
    "valid_features_f = df_valid[feature_list_f].to_numpy()\n",
    "valid_target_f = df_valid['rank_t+1'].to_numpy()\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1).fit(features_f, target_f)\n",
    "hat_valid_target_f = lr.predict(valid_features_f)\n",
    "\n",
    "mae_f = mean_absolute_error(valid_target_f, hat_valid_target_f)\n",
    "\n",
    "print('mae_a: ', mae_f)"
   ]
  }
 ]
}